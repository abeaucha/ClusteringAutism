{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e42b47",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86201845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5845a",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1120a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECTPATH = os.getenv('PROJECTPATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9692b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfe6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_pairs(k):\n",
    "\n",
    "    pairs = []\n",
    "    for pair in product(k, k):\n",
    "        pair = tuple(sorted(pair))\n",
    "        if pair[0] != pair[1]:\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "                \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def prepare_data(clusters, scores, demographics, nk = 2, threshold = 0.5, return_features = False):\n",
    "    \n",
    "    nk_col = 'nk{}'.format(nk)\n",
    "    \n",
    "    # Compute completion rate for the different assessments\n",
    "    nparticipants = scores.shape[0]\n",
    "    completion = dict()\n",
    "    for col, vals in scores.items():\n",
    "        if col != 'Subject_ID':\n",
    "            completion[col] = vals.notna().sum()/nparticipants\n",
    "            \n",
    "    # Filter scales above completion threshold\n",
    "    scales = [col for col, val in completion.items() if val > threshold]        \n",
    "    \n",
    "    # Get the scores for the subset of scales\n",
    "    scores_subset = scores[scales].copy()\n",
    "    scores_subset['Subject_ID'] = scores['Subject_ID']\n",
    "    \n",
    "    # Join scores and cluster information\n",
    "    clusters_scores = clusters.copy()\n",
    "    clusters_scores = (clusters_scores\n",
    "     .rename(columns = {'ID':'file'})\n",
    "     .loc[:, ['file', nk_col]]\n",
    "     .merge(demographics, how = 'left', on = 'file'))\n",
    "\n",
    "    # Filter for POND\n",
    "    clusters_scores = clusters_scores.loc[clusters_scores['Dataset'] == 'POND']\n",
    "\n",
    "    # Clean up IDs for merging\n",
    "    clusters_scores['Subject_ID'] = (clusters_scores['Subject_ID']\n",
    "                                     .str.replace('sub-', '')\n",
    "                                     .astype(int))\n",
    "\n",
    "    # Merge clusters to scores\n",
    "    clusters_scores = (clusters_scores\n",
    "                       .loc[:, ['Subject_ID', nk_col]]\n",
    "                       .merge(scores_subset, on = 'Subject_ID', how = 'left'))\n",
    "\n",
    "    # Keep only complete observations\n",
    "    clusters_scores = clusters_scores.dropna()\n",
    "    \n",
    "    # Create the input matrix and binary targets\n",
    "    X = clusters_scores.drop(['Subject_ID', nk_col], axis = 1)\n",
    "    features = X.columns.to_list()\n",
    "    X = X.to_numpy()\n",
    "    y = np.array(clusters_scores[nk_col], dtype = int)-1\n",
    "    \n",
    "    if return_features:\n",
    "        return X,y,features\n",
    "    else:\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "def run_plsda(X, y):\n",
    "    \n",
    "    # Maximum number of possible components\n",
    "    max_components = X.shape[1]-1\n",
    "    \n",
    "    # Range of model components\n",
    "    component_range = range(2, max_components)\n",
    "\n",
    "    # Iterate over model components\n",
    "    auc = []\n",
    "    for nc in component_range:\n",
    "\n",
    "        # Initiatlize the PLSR module\n",
    "        plsr = PLSRegression(n_components = nc, scale = True)\n",
    "\n",
    "        # Fit the model to the data\n",
    "        plsr.fit(X, y)\n",
    "\n",
    "        # Predict cluster labels\n",
    "        y_pred = plsr.predict(X)\n",
    "\n",
    "        # Clamp the interval since this isn't a proper classifier\n",
    "        y_pred[y_pred > 1.0] = 1.0\n",
    "        y_pred[y_pred < 0] = 0\n",
    "\n",
    "        # Compute AUC\n",
    "        auc.append(roc_auc_score(y, y_pred))\n",
    "\n",
    "    # Store AUC information in data frame\n",
    "    df_auc = pd.DataFrame({'components':component_range, 'auc':auc})\n",
    "\n",
    "    return df_auc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76d0a1",
   "metadata": {},
   "source": [
    "---\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter set ID\n",
    "params_id = 700\n",
    "\n",
    "# Output directory\n",
    "output_dir = 'figure_supplementary'\n",
    "output_dir = os.path.join(PROJECTPATH, 'figures', 'v3', output_dir)\n",
    "\n",
    "# Input directories\n",
    "registration_dir = 'data/human/registration/v3/'\n",
    "pipeline_dir = 'data/human/derivatives/v3/'\n",
    "\n",
    "registration_dir = os.path.join(PROJECTPATH, registration_dir)\n",
    "pipeline_dir = os.path.join(PROJECTPATH, pipeline_dir, str(params_id))\n",
    "\n",
    "# Demographics file\n",
    "demographics = os.path.join(registration_dir, 'subject_info', 'demographics.csv')\n",
    "demographics = pd.read_csv(demographics)\n",
    "\n",
    "# POND clinical scores\n",
    "scores = os.path.join(registration_dir, 'subject_info', 'POND', 'POND_clinical_scores_20230915.csv')\n",
    "scores = pd.read_csv(scores)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Drop columns\n",
    "cols_to_drop = ['Unnamed: 0', 'site', 'SUB_ID', \n",
    "                'DOB', 'PRIMARY_DIAGNOSIS', \n",
    "                'RESEARCH_CONFIRM_DIAG', \n",
    "                'HSHLD_INCOME_STD', \n",
    "                'PRMY_CGVR_STD',\n",
    "               'SWANPDOC', 'TPOCSPDOC']\n",
    "scores = scores.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Drop columns containing the following strings\n",
    "strings_to_drop = ['NSI', 'ETHNCTY', 'EDUC']\n",
    "for s in strings_to_drop:\n",
    "    scores = scores.loc[:, ~scores.columns.str.contains(s)]\n",
    "\n",
    "# Rename the subject ID column for merging\n",
    "scores = scores.rename(columns = {'subject':'Subject_ID'})\n",
    "\n",
    "# Assign NaN to missing values 999 code\n",
    "for col, vals in scores.items():\n",
    "    x = vals.copy()\n",
    "    x[x == 999] = np.nan\n",
    "    scores[col] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816c6e3",
   "metadata": {},
   "source": [
    "---\n",
    "# Pairwise PLS-DA across cluster solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81cbc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of cluster solutions\n",
    "nk_max = 10\n",
    "\n",
    "# Cluster solutions\n",
    "nk_list = list(range(2, nk_max+1))\n",
    "\n",
    "# Completion thresholds\n",
    "thresholds = [0.6, 0.8]\n",
    "\n",
    "# Iterate over cluster solutions\n",
    "df_results = pd.DataFrame()\n",
    "for nk in nk_list:\n",
    "    \n",
    "    # Number of clusters\n",
    "    klist = list(range(1, nk+1))\n",
    "\n",
    "    # Pairs of clusters\n",
    "    kpairs = build_cluster_pairs(k = klist)\n",
    "\n",
    "    # Iterate over cluster pairs\n",
    "    for k in kpairs:\n",
    "\n",
    "        # Get cluster IDs\n",
    "        cluster_ids = ['{}-{}'.format(nk, ki) for ki in k]\n",
    "\n",
    "        # Decrement cluster labels by 1\n",
    "        k = [x-1 for x in k]\n",
    "\n",
    "        # Iterate over completion thresholds\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            # Get inputs and labels\n",
    "            X,y = prepare_data(clusters = clusters, \n",
    "                               scores = scores, \n",
    "                               demographics = demographics,\n",
    "                               nk = nk,\n",
    "                               threshold = threshold, \n",
    "                               return_features = False)\n",
    "\n",
    "            # Filter for participants in the clusters being compared\n",
    "            ind_subset = np.isin(y, k)\n",
    "            X = X[ind_subset,:]\n",
    "            y = y[ind_subset]\n",
    "\n",
    "            # Binarize labels\n",
    "            y[y == k[0]] = 0\n",
    "            y[y == k[1]] = 1\n",
    "            \n",
    "            # Run PLS-DA and catch instances that throw warnings\n",
    "            with warnings.catch_warnings(record = True) as w:\n",
    "                df_results_k = run_plsda(X = X, y = y)\n",
    "                \n",
    "            # Populate data frame instance\n",
    "            df_results_k['nk'] = nk\n",
    "            df_results_k['cluster_id_1'] = cluster_ids[0]\n",
    "            df_results_k['cluster_id_2'] = cluster_ids[1]\n",
    "            df_results_k['threshold'] = threshold\n",
    "            df_results_k['participants'] = X.shape[0]\n",
    "            df_results_k['participants_1'] = sum(y == 0)\n",
    "            df_results_k['participants_2'] = sum(y == 1)\n",
    "            df_results_k['features'] = X.shape[1]\n",
    "            df_results_k['warning'] = True if len(w) > 0 else False\n",
    "            \n",
    "            # Concatenate data frames\n",
    "            df_results = pd.concat([df_results, df_results_k])\n",
    "\n",
    "# Reset index\n",
    "df_results = df_results.reset_index(drop = True)\n",
    "\n",
    "# Export results\n",
    "outfile = 'plsda_results.csv'\n",
    "outfile = os.path.join(output_dir, outfile)\n",
    "df_results.to_csv(outfile, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
