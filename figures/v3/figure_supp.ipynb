{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e42b47",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86201845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5845a",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1120a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECTPATH = os.getenv('PROJECTPATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9692b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfe6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_pairs(k):\n",
    "\n",
    "    pairs = []\n",
    "    for pair in product(k, k):\n",
    "        pair = tuple(sorted(pair))\n",
    "        if pair[0] != pair[1]:\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "                \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def prepare_data(clusters, scores, demographics, nk = 2, threshold = 0.5, return_features = False):\n",
    "    \n",
    "    nk_col = 'nk{}'.format(nk)\n",
    "    \n",
    "    # Compute completion rate for the different assessments\n",
    "    nparticipants = scores.shape[0]\n",
    "    completion = dict()\n",
    "    for col, vals in scores.items():\n",
    "        if col != 'Subject_ID':\n",
    "            completion[col] = vals.notna().sum()/nparticipants\n",
    "            \n",
    "    # Filter scales above completion threshold\n",
    "    scales = [col for col, val in completion.items() if val > threshold]        \n",
    "    \n",
    "    # Get the scores for the subset of scales\n",
    "    scores_subset = scores[scales].copy()\n",
    "    scores_subset['Subject_ID'] = scores['Subject_ID']\n",
    "    \n",
    "    # Join scores and cluster information\n",
    "    clusters_scores = clusters.copy()\n",
    "    clusters_scores = (clusters_scores\n",
    "     .rename(columns = {'ID':'file'})\n",
    "     .loc[:, ['file', nk_col]]\n",
    "     .merge(demographics, how = 'left', on = 'file'))\n",
    "\n",
    "    # Filter for POND\n",
    "    clusters_scores = clusters_scores.loc[clusters_scores['Dataset'] == 'POND']\n",
    "\n",
    "    # Clean up IDs for merging\n",
    "    clusters_scores['Subject_ID'] = (clusters_scores['Subject_ID']\n",
    "                                     .str.replace('sub-', '')\n",
    "                                     .astype(int))\n",
    "\n",
    "    # Merge clusters to scores\n",
    "    clusters_scores = (clusters_scores\n",
    "                       .loc[:, ['Subject_ID', nk_col]]\n",
    "                       .merge(scores_subset, on = 'Subject_ID', how = 'left'))\n",
    "\n",
    "    # Keep only complete observations\n",
    "    clusters_scores = clusters_scores.dropna()\n",
    "    \n",
    "    # Create the input matrix and binary targets\n",
    "    X = clusters_scores.drop(['Subject_ID', nk_col], axis = 1)\n",
    "    features = X.columns.to_list()\n",
    "    X = X.to_numpy()\n",
    "    y = np.array(clusters_scores[nk_col], dtype = int)-1\n",
    "    \n",
    "    if return_features:\n",
    "        return X,y,features\n",
    "    else:\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "def run_plsda(X, y):\n",
    "    \n",
    "    # Maximum number of possible components\n",
    "    max_components = X.shape[1]-1\n",
    "    \n",
    "    # Range of model components\n",
    "    component_range = range(2, max_components)\n",
    "\n",
    "    # Iterate over model components\n",
    "    auc = []\n",
    "    for nc in component_range:\n",
    "\n",
    "        # Initiatlize the PLSR module\n",
    "        plsr = PLSRegression(n_components = nc, scale = True)\n",
    "\n",
    "        # Fit the model to the data\n",
    "        plsr.fit(X, y)\n",
    "\n",
    "        # Predict cluster labels\n",
    "        y_pred = plsr.predict(X)\n",
    "\n",
    "        # Clamp the interval since this isn't a proper classifier\n",
    "        y_pred[y_pred > 1.0] = 1.0\n",
    "        y_pred[y_pred < 0] = 0\n",
    "\n",
    "        # Compute AUC\n",
    "        auc.append(roc_auc_score(y, y_pred))\n",
    "\n",
    "    # Store AUC information in data frame\n",
    "    df_auc = pd.DataFrame({'components':component_range, 'auc':auc})\n",
    "\n",
    "    return df_auc\n",
    "    \n",
    "    \n",
    "def export_scores_loadings(X, y, features, labels):    \n",
    "\n",
    "    labels = pair\n",
    "\n",
    "    # Number of scales\n",
    "    nscales = X.shape[1]\n",
    "\n",
    "    # Initialize the PLSR\n",
    "    plsr = PLSRegression(n_components = 2, scale = True)\n",
    "\n",
    "    # Fit the model\n",
    "    plsr.fit(X, y)\n",
    "\n",
    "    # Get scores and loadings\n",
    "    X_pls = plsr._x_scores\n",
    "    loadings = plsr.x_loadings_\n",
    "\n",
    "    X_PLS_norm = np.sqrt(np.sum(X_pls**2, axis = 1))\n",
    "    loadings_norm = np.sqrt(np.sum(loadings**2, axis = 1))\n",
    "\n",
    "    X_PLS_norm_max = np.max(X_PLS_norm)\n",
    "    loadings_norm_max = np.max(loadings_norm)\n",
    "    scale_factor = X_PLS_norm_max/loadings_norm_max\n",
    "\n",
    "    loadings = loadings*scale_factor\n",
    "\n",
    "    df_pls = pd.DataFrame(X_pls, columns=['x', 'y'])\n",
    "    df_pls['cluster'] = [labels[i] for i in y]\n",
    "\n",
    "    outfile = 'plsda_{}_scores.csv'.format('_'.join(pair))\n",
    "    outfile = os.path.join(output_dir, outfile)\n",
    "    df_pls.to_csv(outfile, index = False)\n",
    "\n",
    "    df_loadings = pd.DataFrame(loadings, columns=['x', 'y'])\n",
    "    df_loadings['features'] = features\n",
    "\n",
    "    outfile = 'plsda_{}_loadings.csv'.format('_'.join(pair))\n",
    "    outfile = os.path.join(output_dir, outfile)\n",
    "    df_loadings.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76d0a1",
   "metadata": {},
   "source": [
    "---\n",
    "# POND analysis\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter set ID\n",
    "params_id = 700\n",
    "\n",
    "# Output directory\n",
    "output_dir = 'figure_supplementary'\n",
    "output_dir = os.path.join(PROJECTPATH, 'figures', 'v3', output_dir)\n",
    "\n",
    "# Input directories\n",
    "registration_dir = 'data/human/registration/v3/'\n",
    "pipeline_dir = 'data/human/derivatives/v3/'\n",
    "\n",
    "registration_dir = os.path.join(PROJECTPATH, registration_dir)\n",
    "pipeline_dir = os.path.join(PROJECTPATH, pipeline_dir, str(params_id))\n",
    "\n",
    "# Demographics file\n",
    "demographics = os.path.join(registration_dir, 'subject_info', 'demographics.csv')\n",
    "demographics = pd.read_csv(demographics)\n",
    "\n",
    "# POND clinical scores\n",
    "scores = os.path.join(registration_dir, 'subject_info', 'POND', 'POND_clinical_scores_20230915.csv')\n",
    "scores = pd.read_csv(scores)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Drop columns\n",
    "cols_to_drop = ['Unnamed: 0', 'site', 'SUB_ID', \n",
    "                'DOB', 'PRIMARY_DIAGNOSIS', \n",
    "                'RESEARCH_CONFIRM_DIAG', \n",
    "                'HSHLD_INCOME_STD', \n",
    "                'PRMY_CGVR_STD',\n",
    "               'SWANPDOC', 'TPOCSPDOC']\n",
    "scores = scores.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Drop columns containing the following strings\n",
    "strings_to_drop = ['NSI', 'ETHNCTY', 'EDUC']\n",
    "for s in strings_to_drop:\n",
    "    scores = scores.loc[:, ~scores.columns.str.contains(s)]\n",
    "\n",
    "# Rename the subject ID column for merging\n",
    "scores = scores.rename(columns = {'subject':'Subject_ID'})\n",
    "\n",
    "# Assign NaN to missing values 999 code\n",
    "for col, vals in scores.items():\n",
    "    x = vals.copy()\n",
    "    x[x == 999] = np.nan\n",
    "    scores[col] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816c6e3",
   "metadata": {},
   "source": [
    "## Pairwise PLS-DA across cluster solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cbc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of cluster solutions\n",
    "nk_max = 10\n",
    "\n",
    "# Cluster solutions\n",
    "nk_list = list(range(2, nk_max+1))\n",
    "\n",
    "# Completion thresholds\n",
    "thresholds = [0.6, 0.8]\n",
    "\n",
    "# Iterate over cluster solutions\n",
    "df_results = pd.DataFrame()\n",
    "for nk in nk_list:\n",
    "    \n",
    "    # Number of clusters\n",
    "    klist = list(range(1, nk+1))\n",
    "\n",
    "    # Pairs of clusters\n",
    "    kpairs = build_cluster_pairs(k = klist)\n",
    "\n",
    "    # Iterate over cluster pairs\n",
    "    for k in kpairs:\n",
    "\n",
    "        # Get cluster IDs\n",
    "        cluster_ids = ['{}-{}'.format(nk, ki) for ki in k]\n",
    "\n",
    "        # Decrement cluster labels by 1\n",
    "        k = [x-1 for x in k]\n",
    "\n",
    "        # Iterate over completion thresholds\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            # Get inputs and labels\n",
    "            X,y = prepare_data(clusters = clusters, \n",
    "                               scores = scores, \n",
    "                               demographics = demographics,\n",
    "                               nk = nk,\n",
    "                               threshold = threshold, \n",
    "                               return_features = False)\n",
    "\n",
    "            # Filter for participants in the clusters being compared\n",
    "            ind_subset = np.isin(y, k)\n",
    "            X = X[ind_subset,:]\n",
    "            y = y[ind_subset]\n",
    "\n",
    "            # Binarize labels\n",
    "            y[y == k[0]] = 0\n",
    "            y[y == k[1]] = 1\n",
    "            \n",
    "            # Run PLS-DA and catch instances that throw warnings\n",
    "            with warnings.catch_warnings(record = True) as w:\n",
    "                df_results_k = run_plsda(X = X, y = y)\n",
    "                \n",
    "            # Populate data frame instance\n",
    "            df_results_k['nk'] = nk\n",
    "            df_results_k['cluster_id_1'] = cluster_ids[0]\n",
    "            df_results_k['cluster_id_2'] = cluster_ids[1]\n",
    "            df_results_k['threshold'] = threshold\n",
    "            df_results_k['participants'] = X.shape[0]\n",
    "            df_results_k['participants_1'] = sum(y == 0)\n",
    "            df_results_k['participants_2'] = sum(y == 1)\n",
    "            df_results_k['features'] = X.shape[1]\n",
    "            df_results_k['warning'] = True if len(w) > 0 else False\n",
    "            \n",
    "            # Concatenate data frames\n",
    "            df_results = pd.concat([df_results, df_results_k])\n",
    "\n",
    "# Reset index\n",
    "df_results = df_results.reset_index(drop = True)\n",
    "\n",
    "# Export results\n",
    "outfile = 'plsda_results.csv'\n",
    "outfile = os.path.join(output_dir, outfile)\n",
    "df_results.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915a033",
   "metadata": {},
   "source": [
    "## Loadings plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54eedb",
   "metadata": {},
   "source": [
    "7-4 (green) vs 7-7 (none)\n",
    "8-7 (green) vs 8-4 (orange)\n",
    "9-4 (green) vs 9-5 (orange)\n",
    "10-4 (green but no mouse match) vs 10-6 (orange but no mouse match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53345ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pairs = [('7-4', '7-7'),\n",
    "('8-7', '8-4'),\n",
    "('9-4', '9-5'),\n",
    "('10-4', '10-6')]\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "palette = ['#4EEE94', '#DF7F4F']\n",
    "\n",
    "xlims = [(-5, 4),\n",
    "        (-4, 4),\n",
    "        (-4, 5),\n",
    "        (-4, 4)]\n",
    "ylims = [(-3, 4),\n",
    "        (-3, 4),\n",
    "        (-3, 4),\n",
    "        (-3, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bae634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(cluster_pairs):\n",
    "# pair = cluster_pairs[0]\n",
    "\n",
    "    nk = int(pair[0].split('-')[0])\n",
    "\n",
    "    k = [int(x.split('-')[1]) for x in pair]\n",
    "    k = [x-1 for x in k]\n",
    "\n",
    "    # Get inputs and labels\n",
    "    X,y,features = prepare_data(clusters = clusters, \n",
    "                       scores = scores, \n",
    "                       demographics = demographics,\n",
    "                       nk = nk,\n",
    "                       threshold = threshold, \n",
    "                       return_features = True)\n",
    "\n",
    "    # Filter for participants in the clusters being compared\n",
    "    ind_subset = np.isin(y, k)\n",
    "    X = X[ind_subset,:]\n",
    "    y = y[ind_subset]\n",
    "\n",
    "    # Binarize labels\n",
    "    y[y == k[0]] = 0\n",
    "    y[y == k[1]] = 1\n",
    "\n",
    "    export_scores_loadings(X = X, y = y, \n",
    "                           features = features, \n",
    "                           labels = pair)\n",
    "    \n",
    "#     outfile = 'plsda_{}_scores.png'.format('_'.join(pair))\n",
    "#     outfile = os.path.join(output_dir, outfile)\n",
    "#     plot_scores_loadings(X = X, y = y, features = features, labels = pair, palette = palette, \n",
    "#                      xlims = [-4, 4], ylims = [-3, 4],\n",
    "#                      outfile = outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8415ad-79e4-4e76-bcdd-9d875603bc2a",
   "metadata": {},
   "source": [
    "---\n",
    "# HBN analysis\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f2c1fee-a0a7-457e-a827-f79552c64b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter set ID\n",
    "params_id = '013'\n",
    "\n",
    "# Output directory\n",
    "output_dir = 'figure_supplementary'\n",
    "output_dir = os.path.join(PROJECTPATH, 'figures', 'v3', output_dir)\n",
    "\n",
    "# Input directories\n",
    "registration_dir = 'data/human/registration/v3/'\n",
    "pipeline_dir = 'data/human/derivatives/v3/'\n",
    "\n",
    "registration_dir = os.path.join(PROJECTPATH, registration_dir)\n",
    "pipeline_dir = os.path.join(PROJECTPATH, pipeline_dir, str(params_id))\n",
    "\n",
    "# Demographics file\n",
    "demographics = os.path.join(registration_dir, 'subject_info', 'demographics.csv')\n",
    "demographics = pd.read_csv(demographics)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Behavioural scores directory \n",
    "behaviour_dir = os.path.join(registration_dir, 'subject_info', 'HBN', 'assessment_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67281998-ae80-4e7a-b9a7-200f2cb27f6a",
   "metadata": {},
   "source": [
    "What are the files that I care about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af4877dc-7fd1-4876-a470-4e31347edd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACE': ['ACE_Score'],\n",
       " 'ARI_P': ['ARI_P_Total_Score'],\n",
       " 'ARI_S': ['ARI_S_Total_Score'],\n",
       " 'ASR': ['ASR_Int_T', 'ASR_Ext_T'],\n",
       " 'ASSQ': ['ASSQ_Total'],\n",
       " 'C3SR': ['C3SR_HY_T', 'C3SR_IN_T'],\n",
       " 'CAARS': ['CAARS_IM_T', 'CAARS_HR_T', 'CAARS_IE_T', 'CAARS_SC_T']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary of files to use\n",
    "dict_files = dict(\n",
    "    ACE = '9994_ACE_20220728.csv', \n",
    "     ARI_P = '9994_ARI_P_20220728.csv',\n",
    "     ARI_S = '9994_ARI_S_20220728.csv',\n",
    "     ASR = '9994_ASR_20220728.csv',\n",
    "     ASSQ = '9994_ASSQ_20220728.csv',\n",
    "     C3SR = '9994_C3SR_20220728.csv',\n",
    "     CAARS = '9994_CAARS_20220728.csv',\n",
    "     CBCL = '9994_CBCL_20220728.csv',\n",
    "     CBCL_Pre = '9994_CBCL_Pre_20220728.csv',\n",
    "     CDI2_P = '9994_CDI2_P_20220728.csv',\n",
    "     CDI2_SR = '9994_CDI2_SR_20220728.csv',\n",
    "     CIS_P = '9994_CIS_P_20220728.csv',\n",
    "     CIS_SR = '9994_CIS_SR_20220728.csv',\n",
    "     DTS = '9994_DTS_20220728.csv',\n",
    "     EVT = '9994_EVT_20220728.csv',\n",
    "     ICU_P = '9994_ICU_P_20220728.csv',\n",
    "     ICU_SR = '9994_ICU_SR_20220728.csv',\n",
    "     KBIT = '9994_KBIT_20220728.csv',\n",
    "     MFQ_P = '9994_MFQ_P_20220728.csv',\n",
    "     MFQ_SR = '9994_MFQ_SR_20220728.csv',\n",
    "     PANAS = '9994_PANAS_20220728.csv',\n",
    "     Quotient = '9994_Quotient_20220728.csv',\n",
    "     RBS = '9994_RBS_20220728.csv',\n",
    "     SAS = '9994_SAS_20220728.csv',\n",
    "     SCARED_P = '9994_SCARED_P_20220728.csv',\n",
    "     SCARED_SR = '9994_SCARED_SR_20220728.csv',\n",
    "     SCQ = '9994_SCQ_20220728.csv',\n",
    "     SDQ = '9994_SDQ_20220728.csv',\n",
    "     SDS = '9994_SDS_20220728.csv',\n",
    "     SRS = '9994_SRS_20220728.csv',\n",
    "     SRS_Pre = '9994_SRS_Pre_20220728.csv',\n",
    "     STAI = '9994_STAI_20220728.csv',\n",
    "     SWAN = '9994_SWAN_20220728.csv',\n",
    "     TRF = '9994_TRF_20220728.csv',\n",
    "     TFR_Pre = '9994_TRF_Pre_20220728.csv',\n",
    "     Vineland = '9994_Vineland_20220728.csv',\n",
    "     WASI = '9994_WASI_20220728.csv',\n",
    "     WHODAS_P = '9994_WHODAS_P_20220728.csv',\n",
    "     WHODAS_SR = '9994_WHODAS_SR_20220728.csv',\n",
    "     WISC = '9994_WISC_20220728.csv',\n",
    "     YSR = '9994_YSR_20220728.csv'\n",
    ")\n",
    "\n",
    "# Dictionary of variables to use in each file\n",
    "dict_vars = dict(ACE = ['ACE_Score'],\n",
    "                  ARI_P = ['ARI_P_Total_Score'],\n",
    "                  ARI_S = ['ARI_S_Total_Score'],\n",
    "                  ASR = ['ASR_Int_T', 'ASR_Ext_T'], # Additional subscales \n",
    "                  ASSQ = ['ASSQ_Total'],\n",
    "                  C3SR = ['C3SR_HY_T', 'C3SR_IN_T'], # Additional subscales\n",
    "                  CAARS = ['CAARS_IM_T', 'CAARS_HR_T', 'CAARS_IE_T', 'CAARS_SC_T'])\n",
    "\n",
    "dict_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ff40feb-a8dd-40c2-b4ac-af4feb253883",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CBCL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(behaviour_dir, val))\n\u001b[1;32m      7\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 8\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[43mdict_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m]\n\u001b[1;32m      9\u001b[0m clusters_behaviours \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(clusters_behaviours, scores, on \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEID\u001b[39m\u001b[38;5;124m'\u001b[39m, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CBCL'"
     ]
    }
   ],
   "source": [
    "# Iterate over files and import variables\n",
    "clusters_behaviours = clusters.copy()\n",
    "for key, val in dict_files.items():\n",
    "# key = 'ACE'\n",
    "# val = dict_files[key]\n",
    "    scores = pd.read_csv(os.path.join(behaviour_dir, val))\n",
    "    scores = scores.iloc[1:]\n",
    "    scores = scores[['EID'] + dict_vars[key]]\n",
    "    clusters_behaviours = pd.merge(clusters_behaviours, scores, on = 'EID', how = 'left')\n",
    "\n",
    "clusters_behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b54d2-ed94-4924-b121-bb297b12a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POND clinical scores\n",
    "scores = os.path.join(registration_dir, 'subject_info', 'POND', 'POND_clinical_scores_20230915.csv')\n",
    "scores = pd.read_csv(scores)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Drop columns\n",
    "cols_to_drop = ['Unnamed: 0', 'site', 'SUB_ID', \n",
    "                'DOB', 'PRIMARY_DIAGNOSIS', \n",
    "                'RESEARCH_CONFIRM_DIAG', \n",
    "                'HSHLD_INCOME_STD', \n",
    "                'PRMY_CGVR_STD',\n",
    "               'SWANPDOC', 'TPOCSPDOC']\n",
    "scores = scores.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Drop columns containing the following strings\n",
    "strings_to_drop = ['NSI', 'ETHNCTY', 'EDUC']\n",
    "for s in strings_to_drop:\n",
    "    scores = scores.loc[:, ~scores.columns.str.contains(s)]\n",
    "\n",
    "# Rename the subject ID column for merging\n",
    "scores = scores.rename(columns = {'subject':'Subject_ID'})\n",
    "\n",
    "# Assign NaN to missing values 999 code\n",
    "for col, vals in scores.items():\n",
    "    x = vals.copy()\n",
    "    x[x == 999] = np.nan\n",
    "    scores[col] = x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
