{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e42b47",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86201845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5845a",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1120a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECTPATH = os.getenv('PROJECTPATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9692b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfe6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_pairs(k):\n",
    "\n",
    "    pairs = []\n",
    "    for pair in product(k, k):\n",
    "        pair = tuple(sorted(pair))\n",
    "        if pair[0] != pair[1]:\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "                \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def prepare_data(clusters, scores, demographics, nk = 2, threshold = 0.5, return_features = False):\n",
    "    \n",
    "    nk_col = 'nk{}'.format(nk)\n",
    "    \n",
    "    # Compute completion rate for the different assessments\n",
    "    nparticipants = scores.shape[0]\n",
    "    completion = dict()\n",
    "    for col, vals in scores.items():\n",
    "        if col != 'Subject_ID':\n",
    "            completion[col] = vals.notna().sum()/nparticipants\n",
    "            \n",
    "    # Filter scales above completion threshold\n",
    "    scales = [col for col, val in completion.items() if val > threshold]        \n",
    "    \n",
    "    # Get the scores for the subset of scales\n",
    "    scores_subset = scores[scales].copy()\n",
    "    scores_subset['Subject_ID'] = scores['Subject_ID']\n",
    "    \n",
    "    # Join scores and cluster information\n",
    "    clusters_scores = clusters.copy()\n",
    "    clusters_scores = (clusters_scores\n",
    "     .rename(columns = {'ID':'file'})\n",
    "     .loc[:, ['file', nk_col]]\n",
    "     .merge(demographics, how = 'left', on = 'file'))\n",
    "\n",
    "    # Filter for POND\n",
    "    clusters_scores = clusters_scores.loc[clusters_scores['Dataset'] == 'POND']\n",
    "\n",
    "    # Clean up IDs for merging\n",
    "    clusters_scores['Subject_ID'] = (clusters_scores['Subject_ID']\n",
    "                                     .str.replace('sub-', '')\n",
    "                                     .astype(int))\n",
    "\n",
    "    # Merge clusters to scores\n",
    "    clusters_scores = (clusters_scores\n",
    "                       .loc[:, ['Subject_ID', nk_col]]\n",
    "                       .merge(scores_subset, on = 'Subject_ID', how = 'left'))\n",
    "\n",
    "    # Keep only complete observations\n",
    "    clusters_scores = clusters_scores.dropna()\n",
    "    \n",
    "    # Create the input matrix and binary targets\n",
    "    X = clusters_scores.drop(['Subject_ID', nk_col], axis = 1)\n",
    "    features = X.columns.to_list()\n",
    "    X = X.to_numpy()\n",
    "    y = np.array(clusters_scores[nk_col], dtype = int)-1\n",
    "    \n",
    "    if return_features:\n",
    "        return X,y,features\n",
    "    else:\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "def run_plsda(X, y):\n",
    "    \n",
    "    # Maximum number of possible components\n",
    "    max_components = X.shape[1]-1\n",
    "    \n",
    "    # Range of model components\n",
    "    component_range = range(2, max_components)\n",
    "\n",
    "    # Iterate over model components\n",
    "    auc = []\n",
    "    for nc in component_range:\n",
    "\n",
    "        # Initiatlize the PLSR module\n",
    "        plsr = PLSRegression(n_components = nc, scale = True)\n",
    "\n",
    "        # Fit the model to the data\n",
    "        plsr.fit(X, y)\n",
    "\n",
    "        # Predict cluster labels\n",
    "        y_pred = plsr.predict(X)\n",
    "\n",
    "        # Clamp the interval since this isn't a proper classifier\n",
    "        y_pred[y_pred > 1.0] = 1.0\n",
    "        y_pred[y_pred < 0] = 0\n",
    "\n",
    "        # Compute AUC\n",
    "        auc.append(roc_auc_score(y, y_pred))\n",
    "\n",
    "    # Store AUC information in data frame\n",
    "    df_auc = pd.DataFrame({'components':component_range, 'auc':auc})\n",
    "\n",
    "    return df_auc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76d0a1",
   "metadata": {},
   "source": [
    "---\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter set ID\n",
    "params_id = 700\n",
    "\n",
    "# Output directory\n",
    "output_dir = 'figure_supplementary'\n",
    "output_dir = os.path.join(PROJECTPATH, 'figures', 'v3', output_dir)\n",
    "\n",
    "# Input directories\n",
    "registration_dir = 'data/human/registration/v3/'\n",
    "pipeline_dir = 'data/human/derivatives/v3/'\n",
    "\n",
    "registration_dir = os.path.join(PROJECTPATH, registration_dir)\n",
    "pipeline_dir = os.path.join(PROJECTPATH, pipeline_dir, str(params_id))\n",
    "\n",
    "# Demographics file\n",
    "demographics = os.path.join(registration_dir, 'subject_info', 'demographics.csv')\n",
    "demographics = pd.read_csv(demographics)\n",
    "\n",
    "# POND clinical scores\n",
    "scores = os.path.join(registration_dir, 'subject_info', 'POND', 'POND_clinical_scores_20230915.csv')\n",
    "scores = pd.read_csv(scores)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Drop columns\n",
    "cols_to_drop = ['Unnamed: 0', 'site', 'SUB_ID', \n",
    "                'DOB', 'PRIMARY_DIAGNOSIS', \n",
    "                'RESEARCH_CONFIRM_DIAG', \n",
    "                'HSHLD_INCOME_STD', \n",
    "                'PRMY_CGVR_STD',\n",
    "               'SWANPDOC', 'TPOCSPDOC']\n",
    "scores = scores.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Drop columns containing the following strings\n",
    "strings_to_drop = ['NSI', 'ETHNCTY', 'EDUC']\n",
    "for s in strings_to_drop:\n",
    "    scores = scores.loc[:, ~scores.columns.str.contains(s)]\n",
    "\n",
    "# Rename the subject ID column for merging\n",
    "scores = scores.rename(columns = {'subject':'Subject_ID'})\n",
    "\n",
    "# Assign NaN to missing values 999 code\n",
    "for col, vals in scores.items():\n",
    "    x = vals.copy()\n",
    "    x[x == 999] = np.nan\n",
    "    scores[col] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816c6e3",
   "metadata": {},
   "source": [
    "---\n",
    "# Pairwise PLS-DA across cluster solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cbc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of cluster solutions\n",
    "nk_max = 10\n",
    "\n",
    "# Cluster solutions\n",
    "nk_list = list(range(2, nk_max+1))\n",
    "\n",
    "# Completion thresholds\n",
    "thresholds = [0.6, 0.8]\n",
    "\n",
    "# Iterate over cluster solutions\n",
    "df_results = pd.DataFrame()\n",
    "for nk in nk_list:\n",
    "    \n",
    "    # Number of clusters\n",
    "    klist = list(range(1, nk+1))\n",
    "\n",
    "    # Pairs of clusters\n",
    "    kpairs = build_cluster_pairs(k = klist)\n",
    "\n",
    "    # Iterate over cluster pairs\n",
    "    for k in kpairs:\n",
    "\n",
    "        # Get cluster IDs\n",
    "        cluster_ids = ['{}-{}'.format(nk, ki) for ki in k]\n",
    "\n",
    "        # Decrement cluster labels by 1\n",
    "        k = [x-1 for x in k]\n",
    "\n",
    "        # Iterate over completion thresholds\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            # Get inputs and labels\n",
    "            X,y = prepare_data(clusters = clusters, \n",
    "                               scores = scores, \n",
    "                               demographics = demographics,\n",
    "                               nk = nk,\n",
    "                               threshold = threshold, \n",
    "                               return_features = False)\n",
    "\n",
    "            # Filter for participants in the clusters being compared\n",
    "            ind_subset = np.isin(y, k)\n",
    "            X = X[ind_subset,:]\n",
    "            y = y[ind_subset]\n",
    "\n",
    "            # Binarize labels\n",
    "            y[y == k[0]] = 0\n",
    "            y[y == k[1]] = 1\n",
    "            \n",
    "            # Run PLS-DA and catch instances that throw warnings\n",
    "            with warnings.catch_warnings(record = True) as w:\n",
    "                df_results_k = run_plsda(X = X, y = y)\n",
    "                \n",
    "            # Populate data frame instance\n",
    "            df_results_k['nk'] = nk\n",
    "            df_results_k['cluster_id_1'] = cluster_ids[0]\n",
    "            df_results_k['cluster_id_2'] = cluster_ids[1]\n",
    "            df_results_k['threshold'] = threshold\n",
    "            df_results_k['participants'] = X.shape[0]\n",
    "            df_results_k['participants_1'] = sum(y == 0)\n",
    "            df_results_k['participants_2'] = sum(y == 1)\n",
    "            df_results_k['features'] = X.shape[1]\n",
    "            df_results_k['warning'] = True if len(w) > 0 else False\n",
    "            \n",
    "            # Concatenate data frames\n",
    "            df_results = pd.concat([df_results, df_results_k])\n",
    "\n",
    "# Reset index\n",
    "df_results = df_results.reset_index(drop = True)\n",
    "\n",
    "# Export results\n",
    "outfile = 'plsda_results.csv'\n",
    "outfile = os.path.join(output_dir, outfile)\n",
    "df_results.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915a033",
   "metadata": {},
   "source": [
    "# Loadings plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54eedb",
   "metadata": {},
   "source": [
    "7-4 (green) vs 7-7 (none)\n",
    "8-7 (green) vs 8-4 (orange)\n",
    "9-4 (green) vs 9-5 (orange)\n",
    "10-4 (green but no mouse match) vs 10-6 (orange but no mouse match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53345ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pairs = [('7-4', '7-7'),\n",
    "('8-7', '8-4'),\n",
    "('9-4', '9-5'),\n",
    "('10-4', '10-6')]\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "palette = ['#4EEE94', '#DF7F4F']\n",
    "\n",
    "xlims = [(-5, 4),\n",
    "        (-4, 4),\n",
    "        (-4, 5),\n",
    "        (-4, 4)]\n",
    "ylims = [(-3, 4),\n",
    "        (-3, 4),\n",
    "        (-3, 4),\n",
    "        (-3, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bae634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(cluster_pairs):\n",
    "# pair = cluster_pairs[0]\n",
    "\n",
    "    nk = int(pair[0].split('-')[0])\n",
    "\n",
    "    k = [int(x.split('-')[1]) for x in pair]\n",
    "    k = [x-1 for x in k]\n",
    "\n",
    "    # Get inputs and labels\n",
    "    X,y,features = prepare_data(clusters = clusters, \n",
    "                       scores = scores, \n",
    "                       demographics = demographics,\n",
    "                       nk = nk,\n",
    "                       threshold = threshold, \n",
    "                       return_features = True)\n",
    "\n",
    "    # Filter for participants in the clusters being compared\n",
    "    ind_subset = np.isin(y, k)\n",
    "    X = X[ind_subset,:]\n",
    "    y = y[ind_subset]\n",
    "\n",
    "    # Binarize labels\n",
    "    y[y == k[0]] = 0\n",
    "    y[y == k[1]] = 1\n",
    "\n",
    "    export_scores_loadings(X = X, y = y, \n",
    "                           features = features, \n",
    "                           labels = pair)\n",
    "    \n",
    "    outfile = 'plsda_{}_scores.png'.format('_'.join(pair))\n",
    "    outfile = os.path.join(output_dir, outfile)\n",
    "    plot_scores_loadings(X = X, y = y, features = features, labels = pair, palette = palette, \n",
    "                     xlims = [-4, 4], ylims = [-3, 4],\n",
    "                     outfile = outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac4fdb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10-4', '10-6')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0712d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_scores_loadings(X, y, features, labels):    \n",
    "\n",
    "    labels = pair\n",
    "\n",
    "    # Number of scales\n",
    "    nscales = X.shape[1]\n",
    "\n",
    "    # Initialize the PLSR\n",
    "    plsr = PLSRegression(n_components = 2, scale = True)\n",
    "\n",
    "    # Fit the model\n",
    "    plsr.fit(X, y)\n",
    "\n",
    "    # Get scores and loadings\n",
    "    X_pls = plsr._x_scores\n",
    "    loadings = plsr.x_loadings_\n",
    "\n",
    "    X_PLS_norm = np.sqrt(np.sum(X_pls**2, axis = 1))\n",
    "    loadings_norm = np.sqrt(np.sum(loadings**2, axis = 1))\n",
    "\n",
    "    X_PLS_norm_max = np.max(X_PLS_norm)\n",
    "    loadings_norm_max = np.max(loadings_norm)\n",
    "    scale_factor = X_PLS_norm_max/loadings_norm_max\n",
    "\n",
    "    loadings = loadings*scale_factor\n",
    "\n",
    "    df_pls = pd.DataFrame(X_pls, columns=['x', 'y'])\n",
    "    df_pls['cluster'] = [labels[i] for i in y]\n",
    "\n",
    "    outfile = 'plsda_{}_scores.csv'.format('_'.join(pair))\n",
    "    outfile = os.path.join(output_dir, outfile)\n",
    "    df_pls.to_csv(outfile, index = False)\n",
    "\n",
    "    df_loadings = pd.DataFrame(loadings, columns=['x', 'y'])\n",
    "    df_loadings['features'] = features\n",
    "\n",
    "    outfile = 'plsda_{}_loadings.csv'.format('_'.join(pair))\n",
    "    outfile = os.path.join(output_dir, outfile)\n",
    "    df_loadings.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b285d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_loadings(X, y, features, labels, palette, xlims, ylims, outfile):\n",
    "\n",
    "    # Number of scales\n",
    "    nscales = X.shape[1]\n",
    "\n",
    "    # Initialize the PLSR\n",
    "    plsr = PLSRegression(n_components = 2, scale = True)\n",
    "\n",
    "    # Fit the model\n",
    "    plsr.fit(X, y)\n",
    "\n",
    "    # Get scores and loadings\n",
    "    X_pls = plsr._x_scores\n",
    "    loadings = plsr.x_loadings_\n",
    "\n",
    "    X_PLS_norm = np.sqrt(np.sum(X_pls**2, axis = 1))\n",
    "    loadings_norm = np.sqrt(np.sum(loadings**2, axis = 1))\n",
    "\n",
    "    X_PLS_norm_max = np.max(X_PLS_norm)\n",
    "    loadings_norm_max = np.max(loadings_norm)\n",
    "    scale_factor = X_PLS_norm_max/loadings_norm_max\n",
    "\n",
    "    loadings = loadings*scale_factor\n",
    "\n",
    "    df_pls = pd.DataFrame(X_pls, columns=['x', 'y'])\n",
    "    df_pls['cluster'] = [labels[i] for i in y]\n",
    "    \n",
    "    p = sns.jointplot(data = df_pls, x = 'x', y = 'y', hue = 'cluster', \n",
    "                      hue_order = labels, palette = palette);\n",
    "    ax = p.ax_joint\n",
    "    for i in range(nscales):\n",
    "        ax.arrow(0, 0, loadings[i,0], loadings[i,1])\n",
    "        ax.text(loadings[i,0], loadings[i,1], features[i], fontsize = 6, horizontalalignment = 'center')\n",
    "\n",
    "    ax.set_xlabel(\"Latent variable 1\", fontsize = 6)\n",
    "    ax.set_ylabel(\"Latent variable 2\", fontsize = 6)\n",
    "    \n",
    "    ax.set_xlim(left = xlims[0], right = xlims[1])\n",
    "    ax.set_ylim(bottom = ylims[0], top = ylims[1])\n",
    "    \n",
    "#     p.fig.subplots_adjust(top = 0.95)\n",
    "    ax.grid()\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles = handles, labels = labels, fontsize = 6)\n",
    "    \n",
    "    ax.tick_params(labelsize = 6)\n",
    "    \n",
    "    plt.savefig(outfile)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "plot_scores_loadings(X = X, y = y, features = features, labels = pair, palette = palette, \n",
    "                     xlims = [-4, 4], ylims = [-3, 4],\n",
    "                     outfile = outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
