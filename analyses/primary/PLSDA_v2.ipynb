{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6653640c",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51830cc0",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfa82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426649b",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57846895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECTPATH = os.getenv('PROJECTPATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb92c9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5c32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(clusters, scores, demographics, nk = 2, threshold = 0.5, return_features = False):\n",
    "    \n",
    "    nk_col = 'nk{}'.format(nk)\n",
    "    \n",
    "    # Compute completion rate for the different assessments\n",
    "    nparticipants = scores.shape[0]\n",
    "    completion = dict()\n",
    "    for col, vals in scores.items():\n",
    "        if col != 'Subject_ID':\n",
    "            completion[col] = vals.notna().sum()/nparticipants\n",
    "            \n",
    "    # Filter scales above completion threshold\n",
    "    scales = [col for col, val in completion.items() if val > threshold]        \n",
    "    \n",
    "    # Get the scores for the subset of scales\n",
    "    scores_subset = scores[scales].copy()\n",
    "    scores_subset['Subject_ID'] = scores['Subject_ID']\n",
    "    \n",
    "    # Join scores and cluster information\n",
    "    clusters_scores = clusters.copy()\n",
    "    clusters_scores = (clusters_scores\n",
    "     .rename(columns = {'ID':'file'})\n",
    "     .loc[:, ['file', nk_col]]\n",
    "     .merge(demographics, how = 'left', on = 'file'))\n",
    "\n",
    "    # Filter for POND\n",
    "    clusters_scores = clusters_scores.loc[clusters_scores['Dataset'] == 'POND']\n",
    "\n",
    "    # Clean up IDs for merging\n",
    "    clusters_scores['Subject_ID'] = (clusters_scores['Subject_ID']\n",
    "                                     .str.replace('sub-', '')\n",
    "                                     .astype(int))\n",
    "\n",
    "    # Merge clusters to scores\n",
    "    clusters_scores = (clusters_scores\n",
    "                       .loc[:, ['Subject_ID', nk_col]]\n",
    "                       .merge(scores_subset, on = 'Subject_ID', how = 'left'))\n",
    "\n",
    "    # Keep only complete observations\n",
    "    clusters_scores = clusters_scores.dropna()\n",
    "    \n",
    "    # Create the input matrix and binary targets\n",
    "    X = clusters_scores.drop(['Subject_ID', nk_col], axis = 1)\n",
    "    features = X.columns.to_list()\n",
    "    X = X.to_numpy()\n",
    "    y = np.array(clusters_scores[nk_col], dtype = int)-1\n",
    "    \n",
    "    if return_features:\n",
    "        return X,y,features\n",
    "    else:\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "def plot_ROC_curves(X, y, scale = True, outfile = None):\n",
    "    \n",
    "    nscales = X.shape[1]\n",
    "    nparticipants = X.shape[0]\n",
    "    \n",
    "    # Maximum number of components\n",
    "    max_components = X.shape[1]-1\n",
    "    \n",
    "    # Iterate over components\n",
    "    component_range = range(2, max_components)\n",
    "    list_auc = []\n",
    "    for nc in component_range:\n",
    "\n",
    "        # Initiatlize the PLSR module\n",
    "        plsr = PLSRegression(n_components = nc, scale = scale)\n",
    "\n",
    "        # Fit the model to the data\n",
    "        plsr.fit(X, y)\n",
    "\n",
    "        # Predict cluster labels\n",
    "        y_pred = plsr.predict(X)\n",
    "\n",
    "        # Clamp the interval since this isn't a proper classifier\n",
    "        y_pred[y_pred > 1.0] = 1.0\n",
    "        y_pred[y_pred < 0] = 0\n",
    "\n",
    "        # Obtain ROC metrics\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y, y_pred)\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(y, y_pred)\n",
    "        list_auc.append(auc)\n",
    "\n",
    "        # Plot the ROC curve for a subset of components\n",
    "        if nc in np.linspace(2, max_components, num = 5, dtype = int):\n",
    "            plt.plot(fpr, tpr, label = '{} (AUC = {:.2f})'.format(nc, auc))\n",
    "\n",
    "    # Complete the ROC curve plot for this threshold\n",
    "    plt.plot(np.linspace(0, 1, 50), np.linspace(0, 1, 50), \n",
    "             linestyle = '--', label = 'Chance level (AUC = 0.5)', \n",
    "            color = \"black\")\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('Input features: {}; Participants: {}'.format(nscales, nparticipants))\n",
    "    plt.legend(title = \"Number of components\")\n",
    "\n",
    "    # Output file\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, dpi = 300)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Close the graphics device\n",
    "    plt.close()\n",
    "    \n",
    "    return list_auc\n",
    "    \n",
    "\n",
    "def plot_scores_loadings(X, y, features = None, scale = True, display_loadings = False, \n",
    "                         labels = None, palette = None, outfile = None):\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [None, None]\n",
    "\n",
    "    # Number of scales and participants\n",
    "    nscales = X.shape[1]\n",
    "    nparticipants = X.shape[0]\n",
    "\n",
    "    # Initiatlize the PLSR module\n",
    "    plsr = PLSRegression(n_components = 2, scale = scale)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    plsr.fit(X, y)\n",
    "\n",
    "    # Get scores and loadings\n",
    "    X_pls = plsr._x_scores\n",
    "    loadings = plsr.x_loadings_\n",
    "\n",
    "    X_PLS_norm = np.sqrt(np.sum(X_pls**2, axis = 1))\n",
    "    loadings_norm = np.sqrt(np.sum(loadings**2, axis = 1))\n",
    "\n",
    "    X_PLS_norm_max = np.max(X_PLS_norm)\n",
    "    loadings_norm_max = np.max(loadings_norm)\n",
    "    scale_factor = X_PLS_norm_max/loadings_norm_max\n",
    "\n",
    "    loadings = loadings*scale_factor\n",
    "\n",
    "    df_pls = pd.DataFrame(X_pls, columns=['x', 'y'])\n",
    "    df_pls['cluster'] = [labels[i] for i in y]\n",
    "\n",
    "    p = sns.jointplot(data = df_pls, x = 'x', y = 'y', hue = 'cluster', \n",
    "                      hue_order = labels, palette = palette);\n",
    "    ax = p.ax_joint\n",
    "    if display_loadings:\n",
    "        if features is None:\n",
    "            raise Exception\n",
    "        for i in range(nscales):\n",
    "            ax.arrow(0, 0, loadings[i,0], loadings[i,1])\n",
    "            ax.text(loadings[i,0], loadings[i,1], features[i], fontsize = 5, horizontalalignment = 'center')\n",
    "\n",
    "    ax.set_xlabel(\"Latent variable 1\")\n",
    "    ax.set_ylabel(\"Latent variable 2\")\n",
    "    p.fig.suptitle('Input features: {}; Participants: {}'.format(nscales, nparticipants))\n",
    "    p.fig.subplots_adjust(top = 0.95)\n",
    "    ax.grid()\n",
    "\n",
    "    # Output file\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, dpi = 300)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Close the graphics device\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def build_cluster_pairs(k):\n",
    "\n",
    "    pairs = []\n",
    "    for pair in product(k, k):\n",
    "        pair = tuple(sorted(pair))\n",
    "        if pair[0] != pair[1]:\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e12d4",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27524a3",
   "metadata": {},
   "source": [
    "Set paths to project directories and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc205f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter set ID\n",
    "params_id = 700\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.join('outputs', 'human_clinical_multivariate', 'v3', str(params_id))\n",
    "if not os.path.exists(output_dir): \n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Input directories\n",
    "registration_dir = 'data/human/registration/v3/'\n",
    "pipeline_dir = 'data/human/derivatives/v3/'\n",
    "\n",
    "registration_dir = os.path.join(PROJECTPATH, registration_dir)\n",
    "pipeline_dir = os.path.join(PROJECTPATH, pipeline_dir, str(params_id))\n",
    "\n",
    "# Demographics file\n",
    "demographics = os.path.join(registration_dir, 'subject_info', 'demographics.csv')\n",
    "demographics = pd.read_csv(demographics)\n",
    "\n",
    "# POND clinical scores\n",
    "scores = os.path.join(registration_dir, 'subject_info', 'POND', 'POND_clinical_scores_20230915.csv')\n",
    "scores = pd.read_csv(scores)\n",
    "\n",
    "# Cluster solutions\n",
    "cluster_dir = os.path.join(pipeline_dir, 'clusters', 'resolution_3.0')\n",
    "cluster_file = os.path.join(cluster_dir, 'clusters.csv')\n",
    "clusters = pd.read_csv(cluster_file)\n",
    "\n",
    "# Drop columns\n",
    "cols_to_drop = ['Unnamed: 0', 'site', 'SUB_ID', \n",
    "                'DOB', 'PRIMARY_DIAGNOSIS', \n",
    "                'RESEARCH_CONFIRM_DIAG', \n",
    "                'HSHLD_INCOME_STD', \n",
    "                'PRMY_CGVR_STD',\n",
    "               'SWANPDOC', 'TPOCSPDOC']\n",
    "scores = scores.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Drop columns containing the following strings\n",
    "strings_to_drop = ['NSI', 'ETHNCTY', 'EDUC']\n",
    "for s in strings_to_drop:\n",
    "    scores = scores.loc[:, ~scores.columns.str.contains(s)]\n",
    "\n",
    "# Rename the subject ID column for merging\n",
    "scores = scores.rename(columns = {'subject':'Subject_ID'})\n",
    "\n",
    "# Assign NaN to missing values 999 code\n",
    "for col, vals in scores.items():\n",
    "    x = vals.copy()\n",
    "    x[x == 999] = np.nan\n",
    "    scores[col] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d254c",
   "metadata": {},
   "source": [
    "# Pairwise comparisons across cluster solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216909c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of cluster solutions\n",
    "nk_max = 10\n",
    "\n",
    "# Cluster solutions\n",
    "nk_list = list(range(2, nk_max+1))\n",
    "\n",
    "# Initialize data frame\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over cluster solutions\n",
    "for nk in nk_list:\n",
    "    \n",
    "    # Number of clusters\n",
    "    klist = list(range(1, nk+1))\n",
    "    \n",
    "    # Pairs of clusters\n",
    "    kpairs = build_cluster_pairs(k = klist)\n",
    "    \n",
    "    # Palette for cluster pairs\n",
    "    palette = {'{}-{}'.format(nk,k):sns.color_palette()[i] for i,k in enumerate(klist)}\n",
    "    \n",
    "    # Iterate over cluster pairs\n",
    "    for k in kpairs:\n",
    "\n",
    "        # Get cluster IDs\n",
    "        cluster_ids = ['{}-{}'.format(nk, ki) for ki in k]\n",
    "        \n",
    "        # Decrement cluster labels by 1\n",
    "        k = [x-1 for x in k]\n",
    "\n",
    "        # Iterate over clinical scale completion thresholds\n",
    "        thresholds = [0.6, 0.8]\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            # Get inputs and labels\n",
    "            X,y,features = prepare_data(clusters = clusters, \n",
    "                               scores = scores, \n",
    "                               demographics = demographics,\n",
    "                               nk = nk,\n",
    "                               threshold = threshold, \n",
    "                                       return_features = True)\n",
    "\n",
    "            # Filter for participants in the select clusters\n",
    "            ind_subset = np.isin(y, k)\n",
    "            X = X[ind_subset,:]\n",
    "            y = y[ind_subset]\n",
    "\n",
    "            # Binarize labels\n",
    "            y[y == k[0]] = 0\n",
    "            y[y == k[1]] = 1\n",
    "\n",
    "            # Plot ROC curves at current threshold\n",
    "            outfile = 'PLSDA_nk{}_{}_{}_ROC_threshold_{}.png'.format(nk, cluster_ids[0], cluster_ids[1], threshold)\n",
    "            outfile = os.path.join(output_dir, outfile)\n",
    "            list_auc = plot_ROC_curves(X = X, y = y, outfile = outfile)\n",
    "            \n",
    "            # Create a data frame with AUC results\n",
    "            df_results_i = pd.DataFrame({'auc':list_auc})\n",
    "            df_results_i['threshold'] = threshold\n",
    "            df_results_i['cluster1'] = cluster_ids[0]\n",
    "            df_results_i['cluster2'] = cluster_ids[1]\n",
    "            df_results_i['nk'] = nk\n",
    "            df_results_i['cluster1_n'] = len(y) - sum(y)\n",
    "            df_results_i['cluster2_n'] = sum(y)\n",
    "\n",
    "            # Concatenate data frames\n",
    "            df_results = pd.concat([df_results, df_results_i], ignore_index = True)\n",
    "\n",
    "            # Plot scores and loadings at threshold\n",
    "            outfile = 'PLSDA_nk{}_{}_{}_scores_threshold_{}.png'.format(nk, cluster_ids[0], cluster_ids[1], threshold)\n",
    "            outfile = os.path.join(output_dir, outfile)\n",
    "            plot_scores_loadings(X = X, y = y, \n",
    "                                 features = features, display_loadings = True, \n",
    "                                 labels = cluster_ids, palette = palette,\n",
    "                                 outfile = outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98e212c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results data frame\n",
    "outfile = 'PLSDA_results.csv'\n",
    "outfile = os.path.join(output_dir, outfile)\n",
    "df_results.to_csv(outfile, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
